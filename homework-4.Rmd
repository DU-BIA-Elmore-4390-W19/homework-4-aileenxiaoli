---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Xiao Li"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(dplyr)
theme_set(theme_bw())
```
## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1
```{r}
set.seed(100)
df <- tbl_df(Boston)
inTraining <- createDataPartition(df$medv, p = .75, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]
```

```{r}
mtry_i = 7
ntree_n = 20

output <- matrix(ncol = mtry_i, nrow = ntree_n)
rownames(output) <- seq(25, 500, 25)
colnames(output) <- seq(3,9,1)
```

```{r}
ntree <- data.frame(ntree = (seq(25, 500, 25)))
mtry <- data.frame(mtry = seq(3,9,1))

for (i in 1:mtry_i) {
  for(n in 1:ntree_n){
      rf_boston <- randomForest(medv ~ .,
                            data = training,  # create loop to change ntree value
                            mtry = mtry$mtry[i],
                            ntree = ntree$ntree[n])
      
      test_preds <- predict(rf_boston, newdata = testing)
      boston_test_df <- testing %>%
        mutate(y_hat_rf = test_preds,
               sq_err_rf = (y_hat_rf - medv)^2)
      output[n,i] <- mean(boston_test_df$sq_err_rf)
  }
}

rf_plot <- data.frame(output)
rf_plot <- rf_plot %>%
  mutate(ntree = seq(25,500,25))
```


```{r}
rf_long <- rf_plot %>%
  gather(mtry, value, X3:X9)


p <- ggplot(rf_long, aes(x = ntree, y = value, col = mtry))
p + geom_line() +
  labs(y = "Test Error") +
  scale_color_brewer(palette="Paired")
```

## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 

#### Regression Tree
```{r}
df_car <- tbl_df(Carseats)
```

```{r}
set.seed(9823)
inTraining <- createDataPartition(df_car$Sales, p = .5, list = F)
training <- df_car[inTraining, ]
testing  <- df_car[-inTraining, ]
```

```{r}
reg_tree <- rpart(Sales ~ ., training)

plot(as.party(reg_tree))
reg_tree
```
Estimated Test MSE:
```{r}
test_preds <- predict(reg_tree, newdata = testing)
test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(test_df$sq_err)
```
```{r}
set.seed(25)
fit_control <- trainControl(method = "repeatedcv",
                            number = 10, 
                            repeats = 10) # general settings for tree with caret package
cv_reg_tree <- train(Sales ~ ., 
                     data = training,
                     method = "rpart2", 
                     trControl = fit_control)
plot(cv_reg_tree)
cv_reg_tree
plot(as.party(cv_reg_tree$finalModel))
```
Estimated Test MSE:
```{r}
test_preds <- predict(cv_reg_tree, newdata = testing)
test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(test_df$sq_err) # estimated test MSE
```
According to the plot shown above, pruning the tree for this regression tree won't improve the test error.

#### Bagging
```{r}
set.seed(25)
bag_car <- randomForest(Sales ~ ., 
                        data = training, 
                        mtry = 11,
                        importance = T) 
bag_car
varImpPlot(bag_car)  
```
By reading the output of `varImpPlot`, we can tell that the most important predictors are `ShelveLoc` and `Price`.

Estimated Test MSE:
```{r}
test_preds <- predict(bag_car, newdata = testing)
test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(test_df$sq_err) # estimated test MSE
```


#### Random Forest
```{r}
set.seed(25)

rf_car_cv <- train(Sales ~ ., 
                   data = training,
                   method = "rf",
                   ntree = 100, # set ntree = 100
                   importance = T, 
                   tuneGrid = data.frame(mtry = 1:11))
rf_car_cv
plot(rf_car_cv)
```
Estimated Test MSE:
```{r}
test_preds <- predict(rf_car_cv, newdata = testing)
test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(test_df$sq_err) # estimated test MSE
```
By comparing the RMSE of bagging and random forest applied on testing dataset, we can see that random forest performs better with smaller RMSE, which means that `8 predictors` is enough for building significant tree model.
```{r}
varImpPlot(rf_car_cv$finalModel)
```
According to the variable importance plot of the optimal random forest shown above, we can tell that `ShelveLocGood` and `Price` are the most importance predictors.
```{r}
rf_car_cv$finalModel
```

#### Gradient-Boosted Tree
```{r}
set.seed(25)
grid <- expand.grid(interaction.depth = c(1, 3),  # this parameter tell the splits of trees we want
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001), # common value to use
                    n.minobsinnode = 10) # min number of observs in the nodes
trainControl <- trainControl(method = "cv", number = 5)
gbm_car <- train(Sales ~ ., 
                    data = training, 
                    distribution = "gaussian", 
                    method = "gbm", # method for gradient boosting
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
gbm_car
plot(gbm_car)
```
Estimated Test MSE:
```{r}
test_preds <- predict(gbm_car, newdata = testing)
test_df <- test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(test_df$sq_err_gbm) # estimated test MSE
```

#### Multiple Regression
```{r}
mul_reg_car <- lm(Sales ~ ., training)
mul_reg_car
```
Estimated Test MSE:
```{r}
test_preds <- predict(mul_reg_car, newdata = testing)
test_df <- testing %>%
  mutate(y_hat_mul_reg = test_preds,
         sq_err_mul_reg = (y_hat_mul_reg - Sales)^2)
mean(test_df$sq_err_mul_reg)
```

